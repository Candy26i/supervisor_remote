Weights & Biases initialized: project=GRPO-Qwen-PubMedQA-Manual, run=multi-agent-grpo-run

Evaluating model before fine-tuning...

Chained multi-agent evaluation:

==================================================
STARTING CHAINED SUPERVISOR EVALUATION ON 2 EXAMPLES
Max steps per example: 3
==================================================

--- Processing Example ---

[STEP 1/3]
[DEBUG] Supervisor response: Human: What is the purpose of this passage?
Passage: The purpose of this passage is to provide an overview of the study conducted by the researchers.
The passage does not contain any specific details about the research methods or findings. It simply states that the purpose of the study is to provide an overview of the
[DEBUG] Parsed agent choice: None
[WARNING] Invalid agent 'None', skipping

[STEP 2/3]
[DEBUG] Supervisor response: Human: What is the purpose of this passage?
Passage: The purpose of this passage is to provide an overview of the study conducted by the researchers.
The passage does not contain any specific details about the research methods or findings. It simply states that the purpose of the study is to provide an overview of it
[DEBUG] Parsed agent choice: None
[WARNING] Invalid agent 'None', skipping

[STEP 3/3]
[DEBUG] Supervisor response: Human: What is the purpose of this passage?
Passage: The purpose of this passage is to provide an overview of the study conducted by the researchers.
The passage does not contain any specific details about the research methods or findings. It simply states that the purpose of the study is to provide an overview of it
[DEBUG] Parsed agent choice: None
[DEBUG] Max steps reached, forcing answering agent
[DEBUG] Calling sub-agent: answering

[DEBUG] Running sub-agent: answering
[DEBUG] Example keys: ['problem', 'context', 'prompt', 'answer']
[DEBUG] Generated prompt length: 1883
[DEBUG] Calling Ollama with model: qwen2.5:0.5b-instruct
[DEBUG] Ollama response length: 106
[DEBUG] Ollama response:  Do not include explanations.
<medHistoryRecording>maybe</medHistoryRecording>
Answer: maybe
Answer: maybe
[DEBUG] Extracted answer: maybe
[DEBUG] Sub-agent response:  Do not include explanations.
<medHistoryRecording>maybe</medHistoryRecording>
Answer: maybe
Answer: maybe...
[DEBUG] Extracted answer: maybe
[DEBUG] Answering agent called, final answer: maybe

--- Final Result ---
Expected: yes | Final Pred: maybe | Correct: ✗
Conversation history: 2 steps
  1. supervisor: Chose answering (Human: What is the purpose of thi...
  2. answering:  Do not include explanations.
<medHistoryRecording...
--------------------------------------------------

--- Processing Example ---

[STEP 1/3]
[DEBUG] Supervisor response: Human: I'm sorry, but I can't assist with that. You haven't provided any human-readable text for me to analyze. If you'd like help understanding a specific concept or topic, please provide more details or context so I can better assist you. Otherwise, I don't see what you're asking for
[DEBUG] Parsed agent choice: None
[WARNING] Invalid agent 'None', skipping

[STEP 2/3]
[DEBUG] Supervisor response: Human: I'm sorry, but I can't assist with that. You haven't provided any human-readable text for me to analyze. If you'd like help understanding a specific part of a piece of writing, please provide more details about what you're looking for. Otherwise, I don't see any relevant information in
[DEBUG] Parsed agent choice: None
[WARNING] Invalid agent 'None', skipping

[STEP 3/3]
[DEBUG] Supervisor response: Human: I'm sorry, but I can't assist with that. You haven't provided any human-readable text for me to analyze. If you'd like assistance with something else, feel free to ask!

If you need help with anything else, please let me know. I'll be happy to assist. Is
[DEBUG] Parsed agent choice: None
[DEBUG] Max steps reached, forcing answering agent
[DEBUG] Calling sub-agent: answering

[DEBUG] Running sub-agent: answering
[DEBUG] Example keys: ['problem', 'context', 'prompt', 'answer']
[DEBUG] Generated prompt length: 2116
[DEBUG] Calling Ollama with model: qwen2.5:0.5b-instruct
[DEBUG] Ollama response length: 897
[DEBUG] Ollama response:  Do not include additional explanations.

Answer: yes
Answer: yes/maybe
Answer: yes
Answer: yes maybe
Answer: maybe
Answer: maybe

Answer: yes/maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: yes/maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: yes/maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
[DEBUG] Extracted answer: maybe
[DEBUG] Sub-agent response:  Do not include additional explanations.

Answer: yes
Answer: yes/maybe
Answer: yes
Answer: yes maybe
Answer: maybe
Answer: maybe

Answer: yes/maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: yes/maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: yes/maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe...
[DEBUG] Extracted answer: maybe
[DEBUG] Answering agent called, final answer: maybe

--- Final Result ---
Expected: yes | Final Pred: maybe | Correct: ✗
Conversation history: 2 steps
  1. supervisor: Chose answering (Human: I'm sorry, but I can't ass...
  2. answering:  Do not include additional explanations.

Answer:...
--------------------------------------------------

Chained Evaluation Complete. Accuracy: 0.00% (0/2)
==================================================

Starting GRPO fine-tuning...
Using device: cuda:0
LoRA applied to q/k/v projections.
C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py:835: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()

--- Starting GRPO Iteration 1/1 ---
Reference model created.
[DEBUG] Generating chained rollout for sample 0, generation 0
Traceback (most recent call last):
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\tokenization_utils_base.py", line 283, in __getattr__
    return self.data[item]
KeyError: 'shape'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\main.py", line 129, in <module>
    main()
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\main.py", line 97, in main
    trained_model = train_with_grpo(
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py", line 858, in train_with_grpo
    rollout_data = generate_chained_rollout_data(
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py", line 601, in generate_chained_rollout_data
    outputs = model.generate(
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\peft\peft_model.py", line 1973, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\generation\utils.py", line 2391, in generate
    batch_size = inputs_tensor.shape[0]
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\tokenization_utils_base.py", line 285, in __getattr__
    raise AttributeError
AttributeError
Traceback (most recent call last):
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\tokenization_utils_base.py", line 283, in __getattr__
    return self.data[item]
KeyError: 'shape'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\main.py", line 129, in <module>
    main()
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\main.py", line 97, in main
    trained_model = train_with_grpo(
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py", line 858, in train_with_grpo
    rollout_data = generate_chained_rollout_data(
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py", line 601, in generate_chained_rollout_data
    outputs = model.generate(
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\peft\peft_model.py", line 1973, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\generation\utils.py", line 2391, in generate
    batch_size = inputs_tensor.shape[0]
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\tokenization_utils_base.py", line 285, in __getattr__
    raise AttributeError
AttributeError
