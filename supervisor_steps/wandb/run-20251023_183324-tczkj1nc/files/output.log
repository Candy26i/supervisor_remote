Weights & Biases initialized: project=GRPO-Qwen-PubMedQA-Manual, run=multi-agent-grpo-run

Evaluating model before fine-tuning...

Chained multi-agent evaluation:

==================================================
STARTING CHAINED SUPERVISOR EVALUATION ON 2 EXAMPLES
Max steps per example: 3
==================================================

--- Processing Example ---

[STEP 1/3]
[DEBUG] Supervisor response: Explanation required: The chosen agent should provide an explanation for why this request is not suitable or how it can be addressed differently without changing the overall message. This will ensure that the supervisor provides a clear and concise response that addresses the specific issue at hand. If the chosen agent is not appropriate, the supervisor should explain their decision
[DEBUG] Parsed agent choice: None
[WARNING] Invalid agent 'None', skipping

[STEP 2/3]
[DEBUG] Supervisor response: Explanation required: The explanation should clearly state which specific agent you will call next based on the provided context. It should not include any irrelevant or extraneous information. Additionally, the explanation should be concise and to the point, requiring minimal interpretation or analysis.
<agent> has been chosen to answer the question. Please provide your
[DEBUG] Parsed agent choice: None
[WARNING] Invalid agent 'None', skipping

[STEP 3/3]
[DEBUG] Supervisor response: Explanation required: The chosen agent should provide an accurate and relevant response based on the given context. The explanation should include the specific details or examples provided in the context to support the choice of the appropriate agent. If no suitable agent can be found, "No suitable agent" should be replied.
In this case, the correct
[DEBUG] Parsed agent choice: None
[DEBUG] Max steps reached, forcing answering agent
[DEBUG] Calling sub-agent: answering

[DEBUG] Running sub-agent: answering
[DEBUG] Example keys: ['problem', 'context', 'prompt', 'answer']
[DEBUG] Generated prompt length: 1883
[DEBUG] Calling Ollama with model: qwen2.5:0.5b-instruct
[DEBUG] Ollama response length: 106
[DEBUG] Ollama response:  Do not include explanations.
<medHistoryRecording>maybe</medHistoryRecording>
Answer: maybe
Answer: maybe
[DEBUG] Extracted answer: maybe
[DEBUG] Sub-agent response:  Do not include explanations.
<medHistoryRecording>maybe</medHistoryRecording>
Answer: maybe
Answer: maybe...
[DEBUG] Extracted answer: maybe
[DEBUG] Answering agent called, final answer: maybe

--- Final Result ---
Expected: yes | Final Pred: maybe | Correct: ✗
Conversation history: 2 steps
  1. supervisor: Chose answering (Explanation required: The chosen ...
  2. answering:  Do not include explanations.
<medHistoryRecording...
--------------------------------------------------

--- Processing Example ---

[STEP 1/3]
[DEBUG] Supervisor response: Explanation: The chosen agent should provide a different perspective on the given topic that does not rely on the previously mentioned information. The explanation should include the specific details of the task and how the chosen agent will address it effectively. The answer should also highlight the importance of the chosen agent in resolving the issue at hand. The reply
[DEBUG] Parsed agent choice: None
[WARNING] Invalid agent 'None', skipping

[STEP 2/3]
[DEBUG] Supervisor response: Explanation: The chosen agent should provide a different perspective on the given topic that does not rely on the previously mentioned information. The explanation should include the specific details of the task and the expected outcome of the request. It should also highlight the importance of following the specified format and content requirements when calling an agent. Additionally, the
[DEBUG] Parsed agent choice: None
[WARNING] Invalid agent 'None', skipping

[STEP 3/3]
[DEBUG] Supervisor response: Agent: context_analysis
Explanation: The patient asked about the cause of their symptoms, so I will use the context analysis agent to provide information about the study design and data collection methods.

Agent: reasoning
Explanation: The patient mentioned that they had hypertension before, but did not mention any other medical conditions.
[DEBUG] Parsed agent choice: context_analysis
[DEBUG] Max steps reached, forcing answering agent
[DEBUG] Calling sub-agent: answering

[DEBUG] Running sub-agent: answering
[DEBUG] Example keys: ['problem', 'context', 'prompt', 'answer']
[DEBUG] Generated prompt length: 2116
[DEBUG] Calling Ollama with model: qwen2.5:0.5b-instruct
[DEBUG] Ollama response length: 897
[DEBUG] Ollama response:  Do not include additional explanations.

Answer: yes
Answer: yes/maybe
Answer: yes
Answer: yes maybe
Answer: maybe
Answer: maybe

Answer: yes/maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: yes/maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: yes/maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
[DEBUG] Extracted answer: maybe
[DEBUG] Sub-agent response:  Do not include additional explanations.

Answer: yes
Answer: yes/maybe
Answer: yes
Answer: yes maybe
Answer: maybe
Answer: maybe

Answer: yes/maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: yes/maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: yes/maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe
Answer: maybe
Answer: yes
Answer: yes Answer: maybe
Answer: maybe...
[DEBUG] Extracted answer: maybe
[DEBUG] Answering agent called, final answer: maybe

--- Final Result ---
Expected: yes | Final Pred: maybe | Correct: ✗
Conversation history: 2 steps
  1. supervisor: Chose answering (context_analysis
Explanation: The...
  2. answering:  Do not include additional explanations.

Answer:...
--------------------------------------------------

Chained Evaluation Complete. Accuracy: 0.00% (0/2)
==================================================

Starting GRPO fine-tuning...
Using device: cuda:0
LoRA applied to q/k/v projections.
C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py:838: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()

--- Starting GRPO Iteration 1/1 ---
Reference model created.
[DEBUG] Generating chained rollout for sample 0, generation 0
Traceback (most recent call last):
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\tokenization_utils_base.py", line 283, in __getattr__
    return self.data[item]
KeyError: 'shape'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\main.py", line 129, in <module>
    main()
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\main.py", line 97, in main
    trained_model = train_with_grpo(
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py", line 861, in train_with_grpo
    rollout_data = generate_chained_rollout_data(
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py", line 604, in generate_chained_rollout_data
    outputs = model.generate(
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\peft\peft_model.py", line 1973, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\generation\utils.py", line 2391, in generate
    batch_size = inputs_tensor.shape[0]
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\tokenization_utils_base.py", line 285, in __getattr__
    raise AttributeError
AttributeError
Traceback (most recent call last):
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\tokenization_utils_base.py", line 283, in __getattr__
    return self.data[item]
KeyError: 'shape'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\main.py", line 129, in <module>
    main()
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\main.py", line 97, in main
    trained_model = train_with_grpo(
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py", line 861, in train_with_grpo
    rollout_data = generate_chained_rollout_data(
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py", line 604, in generate_chained_rollout_data
    outputs = model.generate(
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\peft\peft_model.py", line 1973, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\generation\utils.py", line 2391, in generate
    batch_size = inputs_tensor.shape[0]
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\tokenization_utils_base.py", line 285, in __getattr__
    raise AttributeError
AttributeError
