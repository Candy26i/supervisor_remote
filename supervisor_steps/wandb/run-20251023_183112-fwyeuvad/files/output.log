Weights & Biases initialized: project=GRPO-Qwen-PubMedQA-Manual, run=multi-agent-grpo-run

Evaluating model before fine-tuning...

Chained multi-agent evaluation:

==================================================
STARTING CHAINED SUPERVISOR EVALUATION ON 2 EXAMPLES
Max steps per example: 3
==================================================

--- Processing Example ---

[STEP 1/3]
[DEBUG] Supervisor response: Agent:' prefix. Do not include the ending sentence of any previous response in the input. To ensure clarity, replace all punctuation marks (:) except for the last one (:) within the response with a dash (-).
Agent: context_analysis
<explanation>: This agent
[DEBUG] Parsed agent choice: context_analysis
[DEBUG] Calling sub-agent: context_analysis

[DEBUG] Running sub-agent: context_analysis
[DEBUG] Example keys: ['problem', 'context', 'prompt', 'answer']
[DEBUG] Generated prompt length: 1824
[DEBUG] Calling Ollama with model: qwen2.5:0.5b-instruct
Traceback (most recent call last):
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\main.py", line 129, in <module>
    main()
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\main.py", line 89, in main
    evaluate_supervisor_chained(model, tokenizer, eval_data, device, max_steps=3)
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py", line 982, in evaluate_supervisor_chained
    sub_text, pred = run_subagent(agent, ex, conversation_history)
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py", line 269, in run_subagent
    text = call_ollama(OLLAMA_MODEL, messages)
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py", line 240, in call_ollama
    outputs = model.generate(
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\generation\utils.py", line 2623, in generate
    result = self._sample(
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\generation\utils.py", line 3595, in _sample
    while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\generation\utils.py", line 2782, in _has_unfinished_sequences
    elif this_peer_finished:
KeyboardInterrupt
Traceback (most recent call last):
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\main.py", line 129, in <module>
    main()
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\main.py", line 89, in main
    evaluate_supervisor_chained(model, tokenizer, eval_data, device, max_steps=3)
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py", line 982, in evaluate_supervisor_chained
    sub_text, pred = run_subagent(agent, ex, conversation_history)
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py", line 269, in run_subagent
    text = call_ollama(OLLAMA_MODEL, messages)
  File "C:\Users\madis\Desktop\supervisor_remote\supervisor_steps\funtions.py", line 240, in call_ollama
    outputs = model.generate(
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\generation\utils.py", line 2623, in generate
    result = self._sample(
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\generation\utils.py", line 3595, in _sample
    while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):
  File "C:\Users\madis\anaconda3\envs\openthought\lib\site-packages\transformers\generation\utils.py", line 2782, in _has_unfinished_sequences
    elif this_peer_finished:
KeyboardInterrupt
